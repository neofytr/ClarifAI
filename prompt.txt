class EnhancedSearchEngine:
    def __init__(self, llm_manager):
        self.llm_manager = llm_manager
        self.search_cache = {}
        self.cache_duration = 3600  # 1 hour
        
        # Enhanced source credibility matrix
        self.credibility_matrix = {
            # Tier 1: Premium fact-checking and authoritative sources
            "snopes.com": {"weight": 0.95, "bias": "neutral", "type": "fact_check", "tier": 1},
            "factcheck.org": {"weight": 0.93, "bias": "neutral", "type": "fact_check", "tier": 1},
            "reuters.com": {"weight": 0.92, "bias": "neutral", "type": "news", "tier": 1},
            "apnews.com": {"weight": 0.91, "bias": "neutral", "type": "news", "tier": 1},
            "nature.com": {"weight": 0.96, "bias": "neutral", "type": "academic", "tier": 1},
            "science.org": {"weight": 0.95, "bias": "neutral", "type": "academic", "tier": 1},
            "nejm.org": {"weight": 0.94, "bias": "neutral", "type": "academic", "tier": 1},
            
            # Tier 2: High-quality sources
            "bbc.com": {"weight": 0.88, "bias": "neutral", "type": "news", "tier": 2},
            "npr.org": {"weight": 0.85, "bias": "slight_left", "type": "news", "tier": 2},
            "pbs.org": {"weight": 0.87, "bias": "neutral", "type": "news", "tier": 2},
            "politifact.com": {"weight": 0.83, "bias": "slight_left", "type": "fact_check", "tier": 2},
            "afp.com": {"weight": 0.86, "bias": "neutral", "type": "news", "tier": 2},
            
            # Tier 3: Mainstream sources
            "washingtonpost.com": {"weight": 0.78, "bias": "left", "type": "news", "tier": 3},
            "nytimes.com": {"weight": 0.80, "bias": "left", "type": "news", "tier": 3},
            "wsj.com": {"weight": 0.82, "bias": "slight_right", "type": "news", "tier": 3},
            "economist.com": {"weight": 0.84, "bias": "slight_right", "type": "news", "tier": 3},
            
            # Tier 4: Lower reliability
            "cnn.com": {"weight": 0.72, "bias": "left", "type": "news", "tier": 4},
            "foxnews.com": {"weight": 0.65, "bias": "right", "type": "news", "tier": 4},
        }
        
        # Enhanced pattern recognition
        self.content_patterns = {
            'high_credibility_indicators': [
                r'peer[- ]reviewed', r'published in.*journal', r'clinical trial', r'meta[- ]analysis',
                r'systematic review', r'scientific study', r'research shows', r'according to.*study',
                r'data from.*survey', r'longitudinal study', r'randomized.*trial', r'evidence[- ]based'
            ],
            'misinformation_indicators': [
                r'doctors hate', r'one weird trick', r'shocking truth', r'they don\'t want you',
                r'big pharma.*hide', r'mainstream media.*ignore', r'wake up.*people',
                r'do your own research', r'question everything', r'\\bhoax\\b', r'false flag',
                r'conspiracy', r'cover[- ]up', r'\\bfake\\b.*news'
            ],
            'verification_indicators': [
                r'verified by', r'confirmed by', r'fact[- ]checked', r'debunked', r'false.*claim',
                r'misleading.*information', r'accurate.*information', r'evidence shows',
                r'studies.*confirm', r'research.*supports', r'experts.*agree'
            ],
            'temporal_indicators': [
                r'breaking', r'just in', r'update', r'developing', r'latest', r'recent',
                r'new study', r'fresh evidence', r'updated.*findings'
            ]
        }
        
        # Domain authority mappings
        self.domain_authority = {
            '.edu': 0.85, '.gov': 0.90, '.org': 0.70, '.mil': 0.88,
            'wikipedia.org': 0.75, 'scholar.google.com': 0.80
        }

    async def multi_stage_search_pipeline(self, content: str) -> Dict[str, Any]:
        """
        Enhanced multi-stage search pipeline with claim extraction and strategic querying
        """
        pipeline_start = time.time()
        
        # Stage 1: Content Analysis and Claim Extraction
        claims_analysis = await self._extract_and_analyze_claims(content)
        
        # Stage 2: Strategic Query Generation with Multiple Approaches
        query_strategies = await self._generate_strategic_queries(content, claims_analysis)
        
        # Stage 3: Parallel Multi-Modal Search
        search_results = await self._execute_parallel_searches(query_strategies)
        
        # Stage 4: Advanced Result Processing and Scoring
        processed_results = await self._advanced_result_processing(search_results, content, claims_analysis)
        
        # Stage 5: Context-Aware Source Selection
        final_sources = await self._context_aware_source_selection(processed_results, claims_analysis)
        
        return {
            "claims_analysis": claims_analysis,
            "search_strategies": query_strategies,
            "total_results_found": len(search_results),
            "processed_results": len(processed_results),
            "final_sources": final_sources,
            "pipeline_duration": round(time.time() - pipeline_start, 2),
            "confidence_metrics": self._calculate_search_confidence(final_sources)
        }

    async def _extract_and_analyze_claims(self, content: str) -> Dict[str, Any]:
        """Extract and categorize factual claims from content"""
        claims_prompt = f"""
        Analyze this content and extract distinct factual claims that can be verified or fact-checked.
        Categorize each claim and assess its verifiability.

        Content: "{content}"

        Return a JSON object with this structure:
        {{
            "primary_claims": [
                {{
                    "claim": "specific factual assertion",
                    "category": "scientific|political|historical|statistical|medical|general",
                    "verifiability": "high|medium|low",
                    "specificity": "specific|vague|ambiguous",
                    "temporal_relevance": "current|historical|timeless"
                }}
            ],
            "supporting_claims": ["claim1", "claim2"],
            "content_type": "news|opinion|academic|social_media|advertisement|other",
            "dominant_topic": "brief topic description",
            "urgency_level": "high|medium|low",
            "controversy_potential": "high|medium|low"
        }}
        """
        
        try:
            response = await self.llm_manager.generate_response(claims_prompt, max_tokens=800, temperature=0.1)
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                claims_data = json.loads(json_match.group())
                # Add content fingerprint for caching
                claims_data["content_hash"] = hashlib.md5(content.encode()).hexdigest()[:16]
                return claims_data
        except Exception as e:
            print(f"Claims extraction error: {e}")
        
        # Fallback analysis
        return self._fallback_claims_analysis(content)

    async def _generate_strategic_queries(self, content: str, claims_analysis: Dict[str, Any]) -> Dict[str, List[str]]:
        """Generate multiple types of strategic search queries"""
        strategies = {
            "verification_queries": [],
            "contradiction_queries": [],
            "source_validation_queries": [],
            "context_queries": [],
            "expert_opinion_queries": []
        }
        
        # Generate queries for each strategy
        strategy_prompts = {
            "verification_queries": f"""
            Generate 4-5 search queries to VERIFY these claims. Focus on finding authoritative sources that confirm or deny:
            
            Claims: {[claim['claim'] for claim in claims_analysis.get('primary_claims', [])]}
            Topic: {claims_analysis.get('dominant_topic', 'general')}
            
            Return only the queries, one per line:
            """,
            
            "contradiction_queries": f"""
            Generate 3-4 search queries to find CONTRADICTORY information or debunking of these claims:
            
            Claims: {[claim['claim'] for claim in claims_analysis.get('primary_claims', [])]}
            
            Use terms like: debunked, false, myth, incorrect, misleading
            Return only the queries, one per line:
            """,
            
            "source_validation_queries": f"""
            Generate 3-4 queries to validate the ORIGINAL SOURCES mentioned in this content:
            
            Content: "{content[:400]}"
            
            Focus on finding the original studies, reports, or authorities cited.
            Return only the queries, one per line:
            """,
            
            "expert_opinion_queries": f"""
            Generate 3-4 queries to find EXPERT OPINIONS on this topic:
            
            Topic: {claims_analysis.get('dominant_topic', 'general')}
            Category: {claims_analysis.get('primary_claims', [{}])[0].get('category', 'general') if claims_analysis.get('primary_claims') else 'general'}
            
            Target experts, institutions, and authoritative bodies.
            Return only the queries, one per line:
            """
        }
        
        # Execute query generation in parallel
        tasks = [
            self._generate_query_set(prompt, strategy)
            for strategy, prompt in strategy_prompts.items()
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, (strategy, result) in enumerate(zip(strategy_prompts.keys(), results)):
            if isinstance(result, list):
                strategies[strategy] = result
            else:
                strategies[strategy] = self._fallback_queries(content, strategy)
        
        return strategies

    async def _generate_query_set(self, prompt: str, strategy: str) -> List[str]:
        """Generate a set of queries for a specific strategy"""
        try:
            response = await self.llm_manager.generate_response(prompt, max_tokens=300, temperature=0.3)
            queries = [q.strip() for q in response.split('\n') if q.strip() and len(q.strip()) > 10]
            return queries[:5]  # Limit to 5 queries per strategy
        except Exception as e:
            print(f"Query generation error for {strategy}: {e}")
            return []

    async def _execute_parallel_searches(self, query_strategies: Dict[str, List[str]]) -> List[Dict[str, Any]]:
        """Execute all search queries in parallel with rate limiting"""
        all_queries = []
        for strategy, queries in query_strategies.items():
            for query in queries:
                all_queries.append({"query": query, "strategy": strategy})
        
        # Rate limiting: batch searches
        batch_size = 8
        all_results = []
        
        for i in range(0, len(all_queries), batch_size):
            batch = all_queries[i:i + batch_size]
            
            async with aiohttp.ClientSession() as session:
                tasks = [
                    self._enhanced_single_search(session, item["query"], item["strategy"])
                    for item in batch
                ]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for results in batch_results:
                    if isinstance(results, list):
                        all_results.extend(results)
            
            # Rate limiting delay
            if i + batch_size < len(all_queries):
                await asyncio.sleep(1)
        
        return all_results

    async def _enhanced_single_search(self, session: aiohttp.ClientSession, query: str, strategy: str) -> List[Dict[str, Any]]:
        """Enhanced single search with strategy-specific parameters"""
        try:
            # Check cache first
            cache_key = hashlib.md5(f"{query}_{strategy}".encode()).hexdigest()
            if cache_key in self.search_cache:
                cached_result, timestamp = self.search_cache[cache_key]
                if time.time() - timestamp < self.cache_duration:
                    return cached_result
            
            # Strategy-specific search parameters
            search_params = self._get_strategy_search_params(strategy, query)
            
            async with session.get(
                "https://www.googleapis.com/customsearch/v1",
                params=search_params,
                timeout=25
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    results = []
                    
                    for item in data.get('items', []):
                        result = await self._create_enhanced_result_object(item, query, strategy)
                        if result:
                            results.append(result)
                    
                    # Cache results
                    self.search_cache[cache_key] = (results, time.time())
                    return results
                    
        except Exception as e:
            print(f"Enhanced search error for '{query}' ({strategy}): {e}")
        
        return []

    def _get_strategy_search_params(self, strategy: str, query: str) -> Dict[str, Any]:
        """Get search parameters optimized for specific strategies"""
        base_params = {
            'key': GOOGLE_API_KEY,
            'cx': GOOGLE_CSE_ID,
            'q': query,
            'num': 10,
            'safe': 'medium'
        }
        
        # Strategy-specific modifications
        if strategy == "verification_queries":
            base_params.update({
                'dateRestrict': 'y3',  # Last 3 years
                'siteSearch': 'site:edu OR site:gov OR site:org'
            })
        elif strategy == "contradiction_queries":
            base_params.update({
                'dateRestrict': 'y5',
                'q': f"{query} debunked OR false OR myth"
            })
        elif strategy == "expert_opinion_queries":
            base_params.update({
                'dateRestrict': 'y2',
                'siteSearch': 'site:edu OR site:gov'
            })
        elif strategy == "source_validation_queries":
            base_params.update({
                'dateRestrict': 'y10'
            })
        
        return base_params

    async def _create_enhanced_result_object(self, item: Dict, query: str, strategy: str) -> Optional[Dict[str, Any]]:
        """Create an enhanced result object with comprehensive metadata"""
        try:
            url = item.get('link', '')
            domain = urlparse(url).netloc.lower()
            
            # Skip low-quality domains
            if any(skip in domain for skip in ['pinterest.com', 'quora.com', 'yahoo.com/answers']):
                return None
            
            title = item.get('title', '')
            snippet = item.get('snippet', '')
            content_text = f"{title} {snippet}".lower()
            
            result = {
                "title": title,
                "link": url,
                "snippet": snippet,
                "domain": domain,
                "query_used": query,
                "search_strategy": strategy,
                "timestamp": datetime.now().isoformat(),
                
                # Enhanced credibility analysis
                "credibility_score": self._calculate_enhanced_credibility(url, content_text),
                "source_metadata": self._analyze_source_metadata(item, domain),
                
                # Content analysis
                "content_indicators": self._comprehensive_content_analysis(content_text),
                
                # Relevance scoring
                "relevance_metrics": self._calculate_relevance_metrics(content_text, query, strategy),
                
                # Temporal analysis
                "temporal_indicators": self._extract_temporal_indicators(content_text, item),
                
                # Bias and perspective analysis
                "perspective_analysis": self._analyze_perspective_bias(content_text, domain)
            }
            
            return result
            
        except Exception as e:
            print(f"Result object creation error: {e}")
            return None

    def _calculate_enhanced_credibility(self, url: str, content_text: str) -> Dict[str, float]:
        """Calculate multi-dimensional credibility score"""
        domain = urlparse(url).netloc.lower()
        
        # Base credibility from domain
        base_score = 0.4
        source_type = "other"
        bias_rating = "unknown"
        
        # Check credibility matrix
        for source_domain, metadata in self.credibility_matrix.items():
            if source_domain in domain:
                base_score = metadata["weight"]
                source_type = metadata["type"]
                bias_rating = metadata["bias"]
                break
        
        # Check domain authority patterns
        for authority_pattern, score in self.domain_authority.items():
            if authority_pattern in domain:
                base_score = max(base_score, score)
                break
        
        # Content-based credibility adjustments
        content_score = self._analyze_content_credibility(content_text)
        
        # Combine scores
        final_score = (base_score * 0.7) + (content_score * 0.3)
        
        return {
            "overall_score": round(final_score, 3),
            "domain_score": round(base_score, 3),
            "content_score": round(content_score, 3),
            "source_type": source_type,
            "bias_rating": bias_rating
        }

    def _analyze_content_credibility(self, content_text: str) -> float:
        """Analyze content for credibility indicators"""
        score = 0.5  # Neutral starting point
        
        # Positive indicators
        for pattern in self.content_patterns['high_credibility_indicators']:
            if re.search(pattern, content_text, re.IGNORECASE):
                score += 0.05
        
        # Negative indicators
        for pattern in self.content_patterns['misinformation_indicators']:
            if re.search(pattern, content_text, re.IGNORECASE):
                score -= 0.1
        
        # Verification indicators
        for pattern in self.content_patterns['verification_indicators']:
            if re.search(pattern, content_text, re.IGNORECASE):
                score += 0.03
        
        return max(0.0, min(1.0, score))

    async def _advanced_result_processing(self, search_results: List[Dict[str, Any]], 
                                        content: str, claims_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Advanced processing with deduplication, clustering, and enhanced scoring"""
        if not search_results:
            return []
        
        # Stage 1: Deduplication with semantic similarity
        deduplicated = self._semantic_deduplication(search_results)
        
        # Stage 2: Cluster similar results
        clustered_results = self._cluster_similar_results(deduplicated)
        
        # Stage 3: Enhanced relevance scoring
        scored_results = await self._calculate_advanced_relevance_scores(
            clustered_results, content, claims_analysis
        )
        
        # Stage 4: Diversity optimization
        diversified_results = self._optimize_result_diversity(scored_results)
        
        return diversified_results

    def _semantic_deduplication(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove semantically similar results"""
        seen_domains = set()
        seen_titles = set()
        deduplicated = []
        
        # Sort by credibility first
        results_sorted = sorted(results, key=lambda x: x.get('credibility_score', {}).get('overall_score', 0), reverse=True)
        
        for result in results_sorted:
            domain = result.get('domain', '')
            title = result.get('title', '').lower()
            
            # Skip if we already have a result from this domain (unless it's a high-authority domain)
            if domain in seen_domains and result.get('credibility_score', {}).get('overall_score', 0) < 0.8:
                continue
            
            # Skip if we have a very similar title
            title_words = set(title.split())
            is_similar = any(
                len(title_words.intersection(set(seen_title.split()))) / max(len(title_words), len(seen_title.split())) > 0.8
                for seen_title in seen_titles
            )
            
            if not is_similar:
                deduplicated.append(result)
                seen_domains.add(domain)
                seen_titles.add(title)
        
        return deduplicated

    async def _context_aware_source_selection(self, processed_results: List[Dict[str, Any]], 
                                            claims_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Select sources using advanced context awareness"""
        if not processed_results:
            return []
        
        selection_prompt = f"""
        You are a professional fact-checker selecting the most valuable sources for verification.
        
        CLAIMS TO VERIFY:
        {json.dumps([claim['claim'] for claim in claims_analysis.get('primary_claims', [])], indent=2)}
        
        CONTENT TYPE: {claims_analysis.get('content_type', 'unknown')}
        CONTROVERSY LEVEL: {claims_analysis.get('controversy_potential', 'unknown')}
        
        AVAILABLE SOURCES (showing top candidates):
        {self._format_sources_for_selection(processed_results[:20])}
        
        Select and rank the most valuable sources considering:
        1. Direct relevance to specific claims
        2. Source credibility and authority
        3. Complementary perspectives (pro/con evidence)
        4. Recency and temporal relevance
        5. Source diversity (avoid echo chambers)
        
        Return a JSON array of source indices (0-based) in order of value for fact-checking:
        {{"selected_indices": [1, 5, 12, 8, ...]}}
        
        Select 8-12 sources maximum.
        """
        
        try:
            response = await self.llm_manager.generate_response(selection_prompt, max_tokens=400, temperature=0.1)
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            
            if json_match:
                selection_data = json.loads(json_match.group())
                indices = selection_data.get('selected_indices', [])
                
                selected_sources = []
                for idx in indices:
                    if 0 <= idx < len(processed_results):
                        source = processed_results[idx].copy()
                        source['selection_rank'] = len(selected_sources) + 1
                        selected_sources.append(source)
                
                return selected_sources[:12]
                
        except Exception as e:
            print(f"Context-aware selection error: {e}")
        
        # Fallback to score-based selection
        return sorted(processed_results, 
                     key=lambda x: x.get('relevance_metrics', {}).get('total_score', 0), 
                     reverse=True)[:10]

    def _format_sources_for_selection(self, sources: List[Dict[str, Any]]) -> str:
        """Format sources for LLM selection prompt"""
        formatted = []
        for i, source in enumerate(sources):
            cred = source.get('credibility_score', {})
            rel = source.get('relevance_metrics', {})
            
            formatted.append(f"""
            [{i}] {source.get('title', 'Untitled')}
                Domain: {source.get('domain', 'unknown')} | Strategy: {source.get('search_strategy', 'unknown')}
                Credibility: {cred.get('overall_score', 0):.2f} | Relevance: {rel.get('total_score', 0):.2f}
                Type: {cred.get('source_type', 'unknown')} | Bias: {cred.get('bias_rating', 'unknown')}
                Snippet: {source.get('snippet', 'No snippet')[:100]}...
            """.strip())
        
        return '\n'.join(formatted)

    # Additional helper methods...
    def _fallback_claims_analysis(self, content: str) -> Dict[str, Any]:
        """Fallback claims analysis when LLM fails"""
        return {
            "primary_claims": [{"claim": content[:100], "category": "general", "verifiability": "medium"}],
            "content_type": "unknown",
            "dominant_topic": "general",
            "urgency_level": "medium",
            "controversy_potential": "medium",
            "content_hash": hashlib.md5(content.encode()).hexdigest()[:16]
        }

    def _calculate_search_confidence(self, sources: List[Dict[str, Any]]) -> Dict[str, float]:
        """Calculate confidence metrics for the search results"""
        if not sources:
            return {"overall_confidence": 0.0}
        
        avg_credibility = sum(s.get('credibility_score', {}).get('overall_score', 0) for s in sources) / len(sources)
        source_diversity = len(set(s.get('domain', '') for s in sources)) / len(sources)
        fact_check_ratio = len([s for s in sources if s.get('credibility_score', {}).get('source_type') == 'fact_check']) / len(sources)
        
        overall_confidence = (avg_credibility * 0.5) + (source_diversity * 0.3) + (fact_check_ratio * 0.2)
        
        return {
            "overall_confidence": round(overall_confidence, 3),
            "average_credibility": round(avg_credibility, 3),
            "source_diversity": round(source_diversity, 3),
            "fact_check_ratio": round(fact_check_ratio, 3)
        }


class AdvancedFactChecker:
    def __init__(self):
        self.llm_manager = LLMManager()
        self.image_processor = AdvancedImageProcessor()
        self.search_engine = EnhancedSearchEngine(self.llm_manager)
        
        # Enhanced analysis frameworks
        self.verification_frameworks = {
            "scientific": {
                "criteria": ["peer_review", "replication", "methodology", "sample_size", "statistical_significance"],
                "weight_distribution": [0.25, 0.20, 0.20, 0.15, 0.20]
            },
            "news": {
                "criteria": ["source_diversity", "primary_sources", "expert_quotes", "contradictory_evidence", "temporal_relevance"],
                "weight_distribution": [0.20, 0.25, 0.20, 0.20, 0.15]
            },
            "statistical": {
                "criteria": ["data_source", "methodology", "sample_representativeness", "statistical_methods", "confidence_intervals"],
                "weight_distribution": [0.25, 0.20, 0.20, 0.20, 0.15]
            },
            "historical": {
                "criteria": ["primary_sources", "scholarly_consensus", "archaeological_evidence", "document_authenticity", "cross_references"],
                "weight_distribution": [0.30, 0.25, 0.15, 0.15, 0.15]
            }
        }
        
        # Multi-dimensional verdict system
        self.verdict_criteria = {
            "factual_accuracy": {"weight": 0.35, "description": "Alignment with established facts"},
            "source_reliability": {"weight": 0.25, "description": "Quality and credibility of sources"},
            "evidence_strength": {"weight": 0.20, "description": "Strength and consistency of evidence"},
            "expert_consensus": {"weight": 0.15, "description": "Agreement among domain experts"},
            "methodological_rigor": {"weight": 0.05, "description": "Quality of research methodology"}
        }

    async def comprehensive_fact_check_pipeline(self, content: str, image: Optional[Any] = None) -> Dict[str, Any]:
        """
        Professional multi-stage fact-checking pipeline with advanced analysis
        """
        pipeline_start = time.time()
        
        # Initialize comprehensive analysis result
        analysis_result = {
            "analysis_id": hashlib.md5(f"{content}{time.time()}".encode()).hexdigest()[:16],
            "timestamp": datetime.now().isoformat(),
            "content_metadata": await self._analyze_content_metadata(content),
            "image_analysis": None,
            "pipeline_stages": {},
            "confidence_tracking": {}
        }
        
        try:
            # Stage 1: Content Preprocessing and Enhancement
            stage_start = time.time()
            enhanced_content = await self._preprocess_and_enhance_content(content, image)
            analysis_result["pipeline_stages"]["content_preprocessing"] = {
                "duration": round(time.time() - stage_start, 2),
                "enhanced_content_length": len(enhanced_content["final_content"]),
                "image_processed": enhanced_content["image_processed"]
            } 
             
            print("\n\n")
            print("content preprocessing -> ")
            print(analysis_result)
            
            if enhanced_content["image_analysis"]:
                analysis_result["image_analysis"] = enhanced_content["image_analysis"]
            
            # Stage 2: Advanced Search Pipeline
            stage_start = time.time()
            search_pipeline_result = await self.search_engine.multi_stage_search_pipeline(
                enhanced_content["final_content"]
            )
            analysis_result["pipeline_stages"]["search_pipeline"] = {
                "duration": round(time.time() - stage_start, 2),
                **search_pipeline_result["confidence_metrics"]
            }
            
            print("\n\n")
            print("stage 2 -> ")
            print(analysis_result["pipeline_stages"]["search_pipeline"])
            
            # Stage 3: Multi-Perspective Evidence Analysis
            stage_start = time.time()
            evidence_analysis = await self._multi_perspective_evidence_analysis(
                enhanced_content["final_content"],
                search_pipeline_result["final_sources"],
                search_pipeline_result["claims_analysis"]
            )
            analysis_result["pipeline_stages"]["evidence_analysis"] = {
                "duration": round(time.time() - stage_start, 2),
                "perspectives_analyzed": len(evidence_analysis["perspective_analyses"]),
                "evidence_strength": evidence_analysis["overall_evidence_strength"]
            }
            
            print("\n\n")
            print("stage 3 -> ")
            print(analysis_result["pipeline_stages"]["evidence_analysis"])
            
            # Stage 4: Advanced Credibility Assessment
            stage_start = time.time()
            credibility_assessment = await self._advanced_credibility_assessment(
               enhanced_content["final_content"],
               search_pipeline_result["final_sources"],
               evidence_analysis
           )
            analysis_result["pipeline_stages"]["credibility_assessment"] = {
               "duration": round(time.time() - stage_start, 2),
               "credibility_dimensions": len(credibility_assessment["dimensional_scores"]),
               "overall_credibility": credibility_assessment["overall_credibility_score"]
           }
            
            print("\n\n")
            print("stage 4 -> ")
            print(analysis_result["pipeline_stages"]["credibility_assessment"])
            
           # Stage 5: Expert Domain Analysis
            stage_start = time.time()
            domain_analysis = await self._expert_domain_analysis(
               enhanced_content["final_content"],
               search_pipeline_result["claims_analysis"],
               evidence_analysis
           )
            analysis_result["pipeline_stages"]["domain_analysis"] = {
               "duration": round(time.time() - stage_start, 2),
               "domains_analyzed": len(domain_analysis["domain_assessments"]),
               "expert_consensus_level": domain_analysis["consensus_metrics"]["overall_consensus"]
           }
            
            print("\n\n")
            print("stage 5 -> ")
            print(analysis_result["pipeline_stages"]["domain_analysis"])
            
           # Stage 6: Comprehensive Verdict Generation
            stage_start = time.time()
            final_verdict = await self._generate_comprehensive_verdict(
               enhanced_content["final_content"],
               search_pipeline_result,
               evidence_analysis,
               credibility_assessment,
               domain_analysis
           )
            analysis_result["pipeline_stages"]["verdict_generation"] = {
               "duration": round(time.time() - stage_start, 2),
               "verdict_confidence": final_verdict["confidence_score"],
               "evidence_items_analyzed": final_verdict["evidence_summary"]["total_evidence_items"]
           }
            
            print("\n\n")
            print("stage 6 -> ")
            print(analysis_result["pipeline_stages"]["verdict_generation"])
            
           # Compile final comprehensive result
            analysis_result.update({
               "search_results": search_pipeline_result,
               "evidence_analysis": evidence_analysis,
               "credibility_assessment": credibility_assessment,
               "domain_analysis": domain_analysis,
               "final_verdict": final_verdict,
               "total_pipeline_duration": round(time.time() - pipeline_start, 2),
               "analysis_completeness": self._calculate_analysis_completeness(analysis_result)
           })
            
        except Exception as e:
            analysis_result["error"] = {
               "type": type(e).__name__,
               "message": str(e),
               "stage": "pipeline_execution"
           }
           
        print("\n\ndone\n\n") 
        return analysis_result

    async def _analyze_content_metadata(self, content: str) -> Dict[str, Any]:
       """Analyze content metadata and characteristics"""
       word_count = len(content.split())
       char_count = len(content)
       
       # Detect content type patterns
       content_type_indicators = {
           "news_article": ["breaking", "reported", "according to", "sources say"],
           "social_media": ["#", "@", "RT", "share", "like"],
           "academic": ["study", "research", "journal", "peer-reviewed"],
           "opinion": ["i think", "believe", "opinion", "my view"],
           "advertisement": ["buy", "sale", "discount", "offer", "click here"]
       }
       
       detected_types = []
       for content_type, indicators in content_type_indicators.items():
           if any(indicator.lower() in content.lower() for indicator in indicators):
               detected_types.append(content_type)
       
       return {
           "word_count": word_count,
           "character_count": char_count,
           "estimated_reading_time": round(word_count / 200, 1),  # Average reading speed
           "detected_content_types": detected_types,
           "complexity_score": self._calculate_content_complexity(content),
           "urgency_indicators": self._detect_urgency_indicators(content),
           "emotional_indicators": self._detect_emotional_language(content)
       }

    def _calculate_content_complexity(self, content: str) -> float:
       """Calculate content complexity score"""
       words = content.split()
       if not words:
           return 0.0
       
       # Average word length
       avg_word_length = sum(len(word) for word in words) / len(words)
       
       # Sentence complexity (average words per sentence)
       sentences = content.split('.')
       avg_sentence_length = len(words) / max(len(sentences), 1)
       
       # Technical terms (words > 8 characters)
       technical_terms = len([w for w in words if len(w) > 8]) / len(words)
       
       complexity = (avg_word_length / 10) + (avg_sentence_length / 30) + technical_terms
       return min(1.0, complexity)

    def _detect_urgency_indicators(self, content: str) -> List[str]:
       """Detect urgency indicators in content"""
       urgency_patterns = [
           "breaking", "urgent", "immediate", "now", "today", "just in",
           "developing", "alert", "warning", "critical", "emergency"
       ]
       return [pattern for pattern in urgency_patterns if pattern.lower() in content.lower()]

    def _detect_emotional_language(self, content: str) -> Dict[str, int]:
       """Detect emotional language patterns"""
       emotion_patterns = {
           "anger": ["outraged", "furious", "angry", "mad", "disgusted"],
           "fear": ["terrifying", "scary", "frightening", "alarming", "worried"],
           "excitement": ["amazing", "incredible", "fantastic", "wonderful", "exciting"],
           "sadness": ["tragic", "sad", "heartbreaking", "devastating", "terrible"]
       }
       
       detected_emotions = {}
       for emotion, patterns in emotion_patterns.items():
           count = sum(1 for pattern in patterns if pattern.lower() in content.lower())
           if count > 0:
               detected_emotions[emotion] = count
       
       return detected_emotions

    async def _preprocess_and_enhance_content(self, content: str, image: Optional[Any] = None) -> Dict[str, Any]:
        enhanced_result = {
            "original_content": content,
            "final_content": content,
            "image_processed": False,
            "image_analysis": None,
            "content_enhancements": [],
            "processing_metadata": {
                "timestamp": datetime.now().isoformat(),
                "processing_time": 0.0,
                "success": True,
                "errors": []
            }
        }
        start_time = time.time()
        if image:
            try:
                if not isinstance(image, Image.Image):
                    if hasattr(image, 'read'):
                        image = Image.open(image)
                    elif isinstance(image, (str, Path)):
                        image = Image.open(image)
                    elif isinstance(image, np.ndarray):
                        image = Image.fromarray(image)
                    else:
                        raise ValueError(f"Unsupported image type: {type(image)}")
                text_extraction_result = self.image_processor.extract_text_comprehensive(image)
                image_metadata = self.image_processor.analyze_image_metadata(image)
                image_analysis = {
                    "text_extraction": text_extraction_result,
                    "metadata": image_metadata,
                    "processing_details": {
                        "preprocessing_variants": len(self.image_processor.preprocess_image(image)),
                        "ocr_engines_used": list(self.image_processor.ocr_engines.keys()),
                        "best_extraction_method": text_extraction_result.get("method", "none"),
                        "confidence_score": text_extraction_result.get("confidence", 0.0)
                    }
                }
                enhanced_result["image_analysis"] = image_analysis
                enhanced_result["image_processed"] = True
                extracted_text = text_extraction_result.get("text", "").strip()
                confidence = text_extraction_result.get("confidence", 0.0)
                if extracted_text and len(extracted_text) > 5:
                    if confidence > 50:
                        enhanced_content = f"{content}\n\n--- Extracted Text (High Confidence) ---\n{extracted_text}"
                        enhanced_result["content_enhancements"].append("high_confidence_text_extraction")
                    elif confidence > 20:
                        enhanced_content = f"{content}\n\n--- Extracted Text (Medium Confidence) ---\n{extracted_text}"
                        enhanced_result["content_enhancements"].append("medium_confidence_text_extraction")
                    else:
                        all_extractions = text_extraction_result.get("all_extractions", {})
                        if all_extractions:
                            extraction_summary = "\n".join([
                                f"{method}: {text[:100]}..." 
                                for method, text in all_extractions.items() 
                                if text.strip()
                            ])
                            enhanced_content = f"{content}\n\n--- Multiple Text Extraction Attempts ---\n{extraction_summary}"
                            enhanced_result["content_enhancements"].append("multiple_extraction_attempts")
                        else:
                            enhanced_content = content
                    enhanced_result["final_content"] = enhanced_content
                quality_score = image_metadata.get("quality_score", 0)
                color_analysis = image_metadata.get("color_analysis", {})
                technical_context = []
                if quality_score < 30:
                    technical_context.append(f"Image quality: Low ({quality_score:.1f}/100) - may affect text extraction accuracy")
                elif quality_score > 70:
                    technical_context.append(f"Image quality: High ({quality_score:.1f}/100) - optimal for text extraction")
                brightness = color_analysis.get("brightness", 128)
                contrast = color_analysis.get("contrast", 0)
                if brightness < 80:
                    technical_context.append("Image appears dark - preprocessing applied for better OCR")
                elif brightness > 200:
                    technical_context.append("Image appears bright - preprocessing applied for better OCR")
                if contrast < 30:
                    technical_context.append("Low contrast detected - enhancement algorithms applied")
                if technical_context:
                    enhanced_result["final_content"] += f"\n\n--- Image Processing Notes ---\n" + "\n".join(technical_context)
                    enhanced_result["content_enhancements"].append("technical_context_analysis")
                dominant_colors = color_analysis.get("dominant_colors", [])
                if dominant_colors:
                    color_desc = f"Dominant colors detected: {len(dominant_colors)} color clusters"
                    enhanced_result["final_content"] += f"\n\n--- Visual Analysis ---\n{color_desc}"
                    enhanced_result["content_enhancements"].append("color_analysis")
            except Exception as e:
                error_msg = f"Image processing error: {str(e)}"
                enhanced_result["processing_metadata"]["errors"].append(error_msg)
                enhanced_result["processing_metadata"]["success"] = False
                try:
                    if isinstance(image, Image.Image):
                        basic_info = f"Image detected: {image.size[0]}x{image.size[1]} pixels, {image.mode} mode"
                        enhanced_result["final_content"] += f"\n\n--- Basic Image Info ---\n{basic_info}"
                        enhanced_result["content_enhancements"].append("basic_image_info")
                except:
                    pass
        else:
            content_stats = {
                "length": len(content),
                "words": len(content.split()),
                "lines": len(content.splitlines()),
                "has_urls": bool(re.search(r'https?://', content)),
                "has_emails": bool(re.search(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', content))
            }
            enhanced_result["content_analysis"] = content_stats
            enhanced_result["content_enhancements"].append("content_statistical_analysis")
        enhanced_result["processing_metadata"]["processing_time"] = time.time() - start_time
        return enhanced_result

    async def _multi_perspective_evidence_analysis(self, content: str, sources: List[Dict[str, Any]], 
                                                claims_analysis: Dict[str, Any]) -> Dict[str, Any]:
       """Analyze evidence from multiple perspectives"""
       evidence_prompt = f"""
       As a professional fact-checker, analyze the evidence for these claims from multiple perspectives:

       CLAIMS TO ANALYZE:
       {json.dumps([claim['claim'] for claim in claims_analysis.get('primary_claims', [])], indent=2)}

       AVAILABLE EVIDENCE SOURCES:
       {self._format_sources_for_analysis(sources[:15])}

       Provide a comprehensive analysis considering:
       1. SUPPORTING EVIDENCE - What evidence supports each claim?
       2. CONTRADICTING EVIDENCE - What evidence contradicts each claim?
       3. EXPERT CONSENSUS - What do experts in relevant fields say?
       4. METHODOLOGICAL ANALYSIS - How strong are the research methods?
       5. SOURCE RELIABILITY - How reliable are the sources?

       Return a JSON object with this structure:
       {{
           "perspective_analyses": {{
               "supporting_evidence": {{
                   "strength": "strong|moderate|weak",
                   "key_sources": ["source1", "source2"],
                   "evidence_quality": "high|medium|low",
                   "summary": "brief summary"
               }},
               "contradicting_evidence": {{
                   "strength": "strong|moderate|weak",
                   "key_sources": ["source1", "source2"],
                   "evidence_quality": "high|medium|low",
                   "summary": "brief summary"
               }},
               "expert_consensus": {{
                   "consensus_level": "strong|moderate|weak|none",
                   "expert_sources": ["source1", "source2"],
                   "disagreement_areas": ["area1", "area2"]
               }},
               "methodological_assessment": {{
                   "overall_quality": "high|medium|low",
                   "strengths": ["strength1", "strength2"],
                   "limitations": ["limitation1", "limitation2"]
               }}
           }},
           "overall_evidence_strength": "strong|moderate|weak",
           "confidence_level": 0.85,
           "evidence_gaps": ["gap1", "gap2"]
       }}
       """
       
       try:
           response = await self.llm_manager.generate_response(evidence_prompt, max_tokens=1200, temperature=0.1)
           json_match = re.search(r'\{.*\}', response, re.DOTALL)
           
           if json_match:
               evidence_analysis = json.loads(json_match.group())
               evidence_analysis["analysis_timestamp"] = datetime.now().isoformat()
               evidence_analysis["sources_analyzed"] = len(sources)
               return evidence_analysis
               
       except Exception as e:
           print(f"Evidence analysis error: {e}")
       
       # Fallback analysis
       return self._fallback_evidence_analysis(sources)

    def _format_sources_for_analysis(self, sources: List[Dict[str, Any]]) -> str:
       """Format sources for evidence analysis"""
       formatted = []
       for i, source in enumerate(sources):
           cred = source.get('credibility_score', {})
           formatted.append(f"""
           Source {i+1}: {source.get('title', 'Untitled')}
           Domain: {source.get('domain', 'unknown')}
           Credibility: {cred.get('overall_score', 0):.2f} ({cred.get('source_type', 'unknown')})
           Strategy: {source.get('search_strategy', 'unknown')}
           Content: {source.get('snippet', 'No content')[:200]}...
           """.strip())
       
       return '\n\n'.join(formatted)

    def _fallback_evidence_analysis(self, sources: List[Dict[str, Any]]) -> Dict[str, Any]:
       """Fallback evidence analysis when LLM fails"""
       avg_credibility = sum(s.get('credibility_score', {}).get('overall_score', 0) for s in sources) / max(len(sources), 1)
       
       return {
           "perspective_analyses": {
               "supporting_evidence": {
                   "strength": "moderate",
                   "key_sources": [s.get('domain', 'unknown') for s in sources[:3]],
                   "evidence_quality": "medium",
                   "summary": "Analysis based on available sources"
               },
               "contradicting_evidence": {
                   "strength": "weak",
                   "key_sources": [],
                   "evidence_quality": "low",
                   "summary": "Limited contradicting evidence found"
               },
               "expert_consensus": {
                   "consensus_level": "moderate",
                   "expert_sources": [],
                   "disagreement_areas": []
               },
               "methodological_assessment": {
                   "overall_quality": "medium",
                   "strengths": ["Multiple sources consulted"],
                   "limitations": ["Limited expert analysis"]
               }
           },
           "overall_evidence_strength": "moderate",
           "confidence_level": avg_credibility,
           "evidence_gaps": ["Limited expert analysis", "Methodological assessment needed"]
       }

    async def _advanced_credibility_assessment(self, content: str, sources: List[Dict[str, Any]], 
                                            evidence_analysis: Dict[str, Any]) -> Dict[str, Any]:
       """Advanced multi-dimensional credibility assessment"""
       credibility_prompt = f"""
       Conduct a professional credibility assessment of this content and its supporting evidence:

       CONTENT TO ASSESS: "{content[:500]}..."

       EVIDENCE ANALYSIS SUMMARY:
       - Supporting Evidence Strength: {evidence_analysis.get('perspective_analyses', {}).get('supporting_evidence', {}).get('strength', 'unknown')}
       - Contradicting Evidence Strength: {evidence_analysis.get('perspective_analyses', {}).get('contradicting_evidence', {}).get('strength', 'unknown')}
       - Expert Consensus Level: {evidence_analysis.get('perspective_analyses', {}).get('expert_consensus', {}).get('consensus_level', 'unknown')}

       SOURCE QUALITY SUMMARY:
       {self._summarize_source_quality(sources)}

       Assess credibility across multiple dimensions:

       Return a JSON object:
       {{
           "dimensional_scores": {{
               "factual_accuracy": {{"score": 0.85, "justification": "reason"}},
               "source_reliability": {{"score": 0.78, "justification": "reason"}},
               "evidence_consistency": {{"score": 0.92, "justification": "reason"}},
               "expert_validation": {{"score": 0.70, "justification": "reason"}},
               "methodological_rigor": {{"score": 0.65, "justification": "reason"}},
               "temporal_relevance": {{"score": 0.88, "justification": "reason"}}
           }},
           "overall_credibility_score": 0.80,
           "credibility_level": "high|moderate|low",
           "key_strengths": ["strength1", "strength2"],
           "key_weaknesses": ["weakness1", "weakness2"],
           "improvement_recommendations": ["rec1", "rec2"]
       }}
       """
       
       try:
           response = await self.llm_manager.generate_response(credibility_prompt, max_tokens=1000, temperature=0.1)
           json_match = re.search(r'\{.*\}', response, re.DOTALL)
           
           if json_match:
               credibility_assessment = json.loads(json_match.group())
               credibility_assessment["assessment_timestamp"] = datetime.now().isoformat()
               return credibility_assessment
               
       except Exception as e:
           print(f"Credibility assessment error: {e}")
       
       return self._fallback_credibility_assessment(sources)

    def _summarize_source_quality(self, sources: List[Dict[str, Any]]) -> str:
       """Summarize overall source quality"""
       if not sources:
           return "No sources available"
       
       total_sources = len(sources)
       high_cred_sources = len([s for s in sources if s.get('credibility_score', {}).get('overall_score', 0) > 0.8])
       fact_check_sources = len([s for s in sources if s.get('credibility_score', {}).get('source_type') == 'fact_check'])
       academic_sources = len([s for s in sources if s.get('credibility_score', {}).get('source_type') == 'academic'])
       
       return f"""
       Total Sources: {total_sources}
       High Credibility (>0.8): {high_cred_sources} ({high_cred_sources/total_sources*100:.1f}%)
       Fact-Checking Sources: {fact_check_sources}
       Academic Sources: {academic_sources}
       Average Credibility: {sum(s.get('credibility_score', {}).get('overall_score', 0) for s in sources) / total_sources:.2f}
       """

    def _fallback_credibility_assessment(self, sources: List[Dict[str, Any]]) -> Dict[str, Any]:
       """Fallback credibility assessment"""
       avg_credibility = sum(s.get('credibility_score', {}).get('overall_score', 0) for s in sources) / max(len(sources), 1)
       
       return {
           "dimensional_scores": {
               "factual_accuracy": {"score": avg_credibility, "justification": "Based on source credibility"},
               "source_reliability": {"score": avg_credibility, "justification": "Average of source scores"},
               "evidence_consistency": {"score": 0.6, "justification": "Limited analysis available"},
               "expert_validation": {"score": 0.5, "justification": "Insufficient expert sources"},
               "methodological_rigor": {"score": 0.5, "justification": "Cannot assess methodology"},
               "temporal_relevance": {"score": 0.7, "justification": "Recent sources included"}
           },
           "overall_credibility_score": avg_credibility,
           "credibility_level": "moderate" if avg_credibility > 0.6 else "low",
           "key_strengths": ["Multiple sources consulted"],
           "key_weaknesses": ["Limited expert analysis"],
           "improvement_recommendations": ["Seek expert validation", "Review methodology"]
       }

    async def _expert_domain_analysis(self, content: str, claims_analysis: Dict[str, Any], 
                                   evidence_analysis: Dict[str, Any]) -> Dict[str, Any]:
       """Analyze content from expert domain perspectives"""
       domain_categories = claims_analysis.get('primary_claims', [{}])[0].get('category', 'general')
       
       domain_prompt = f"""
       As a domain expert in {domain_categories}, analyze this content and claims:

       CONTENT: "{content[:400]}..."

       CLAIMS: {json.dumps([claim['claim'] for claim in claims_analysis.get('primary_claims', [])], indent=2)}

       EVIDENCE STRENGTH: {evidence_analysis.get('overall_evidence_strength', 'unknown')}

       Provide expert analysis from the perspective of {domain_categories} domain:

       Return JSON:
       {{
           "domain_assessments": {{
               "{domain_categories}": {{
                   "expert_consensus": "strong|moderate|weak|divided",
                   "established_facts": ["fact1", "fact2"],
                   "areas_of_uncertainty": ["area1", "area2"],
                   "methodological_concerns": ["concern1", "concern2"],
                   "domain_specific_credibility": 0.75
               }}
           }},
           "consensus_metrics": {{
               "overall_consensus": 0.70,
               "consensus_strength": "moderate",
               "dissenting_views": ["view1", "view2"]
           }},
           "expert_recommendations": ["rec1", "rec2"],
           "domain_confidence": 0.80
       }}
       """
       
       try:
           response = await self.llm_manager.generate_response(domain_prompt, max_tokens=800, temperature=0.1)
           json_match = re.search(r'\{.*\}', response, re.DOTALL)
           
           if json_match:
               domain_analysis = json.loads(json_match.group())
               domain_analysis["analysis_timestamp"] = datetime.now().isoformat()
               return domain_analysis
               
       except Exception as e:
           print(f"Domain analysis error: {e}")
       
       return self._fallback_domain_analysis(domain_categories)

    def _fallback_domain_analysis(self, domain_category: str) -> Dict[str, Any]:
       """Fallback domain analysis"""
       return {
           "domain_assessments": {
               domain_category: {
                   "expert_consensus": "moderate",
                   "established_facts": [],
                   "areas_of_uncertainty": ["Analysis incomplete"],
                   "methodological_concerns": ["Limited expert input"],
                   "domain_specific_credibility": 0.5
               }
           },
           "consensus_metrics": {
               "overall_consensus": 0.5,
               "consensus_strength": "weak",
               "dissenting_views": []
           },
           "expert_recommendations": ["Seek additional expert validation"],
           "domain_confidence": 0.4
       }

    async def _generate_comprehensive_verdict(self, content: str, search_results: Dict[str, Any],
                                           evidence_analysis: Dict[str, Any], credibility_assessment: Dict[str, Any],
                                           domain_analysis: Dict[str, Any]) -> Dict[str, Any]:
       """Generate comprehensive final verdict with detailed justification"""
       verdict_prompt = f"""
       Generate a comprehensive fact-checking verdict based on this analysis:

       CONTENT ANALYZED: "{content[:300]}..."

       ANALYSIS RESULTS:
       - Search Confidence: {search_results.get('confidence_metrics', {}).get('overall_confidence', 0):.2f}
       - Evidence Strength: {evidence_analysis.get('overall_evidence_strength', 'unknown')}
       - Overall Credibility: {credibility_assessment.get('overall_credibility_score', 0):.2f}
       - Expert Consensus: {domain_analysis.get('consensus_metrics', {}).get('overall_consensus', 0):.2f}

       KEY EVIDENCE SUMMARY:
       Supporting: {evidence_analysis.get('perspective_analyses', {}).get('supporting_evidence', {}).get('strength', 'unknown')}
       Contradicting: {evidence_analysis.get('perspective_analyses', {}).get('contradicting_evidence', {}).get('strength', 'unknown')}

       Generate a professional fact-checking verdict:

       Return JSON:
       {{
           "verdict": "TRUE|MOSTLY_TRUE|MIXED|MOSTLY_FALSE|FALSE|UNVERIFIABLE",
           "confidence_score": 0.85,
           "verdict_explanation": "detailed explanation of the verdict",
           "key_findings": [
               "finding1",
               "finding2",
               "finding3"
           ],
           "evidence_summary": {{
               "total_evidence_items": 15,
               "supporting_evidence_count": 8,
               "contradicting_evidence_count": 3,
               "neutral_evidence_count": 4,
               "evidence_quality_score": 0.78
           }},
           "limitations": [
               "limitation1",
               "limitation2"
           ],
           "recommendations": [
               "recommendation1",
               "recommendation2"
           ],
           "confidence_factors": {{
               "source_quality": 0.80,
               "evidence_consistency": 0.75,
               "expert_consensus": 0.70,
               "methodological_rigor": 0.65
           }}
       }}
       """
       
       try:
           response = await self.llm_manager.generate_response(verdict_prompt, max_tokens=1200, temperature=0.1)
           json_match = re.search(r'\{.*\}', response, re.DOTALL)
           
           if json_match:
               verdict_data = json.loads(json_match.group())
               verdict_data["verdict_timestamp"] = datetime.now().isoformat()
               verdict_data["analysis_completeness"] = self._calculate_verdict_completeness(verdict_data)
               return verdict_data
               
       except Exception as e:
           print(f"Verdict generation error: {e}")
       
       return self._fallback_verdict_generation(credibility_assessment, evidence_analysis)

    def _calculate_verdict_completeness(self, verdict_data: Dict[str, Any]) -> float:
       """Calculate completeness score for the verdict"""
       required_fields = ["verdict", "confidence_score", "verdict_explanation", "key_findings", "evidence_summary"]
       present_fields = sum(1 for field in required_fields if verdict_data.get(field))
       return present_fields / len(required_fields)

    def _fallback_verdict_generation(self, credibility_assessment: Dict[str, Any], 
                                  evidence_analysis: Dict[str, Any]) -> Dict[str, Any]:
       """Fallback verdict generation"""
       overall_credibility = credibility_assessment.get('overall_credibility_score', 0.5)
       evidence_strength = evidence_analysis.get('overall_evidence_strength', 'moderate')
       
       # Simple verdict logic
       if overall_credibility > 0.8 and evidence_strength == 'strong':
           verdict = "MOSTLY_TRUE"
       elif overall_credibility > 0.6 and evidence_strength in ['strong', 'moderate']:
           verdict = "MIXED"
       elif overall_credibility < 0.4:
           verdict = "MOSTLY_FALSE"
       else:
           verdict = "UNVERIFIABLE"
       
       return {
           "verdict": verdict,
           "confidence_score": overall_credibility,
           "verdict_explanation": "Verdict based on automated analysis due to processing limitations",
           "key_findings": [
               f"Overall credibility score: {overall_credibility:.2f}",
               f"Evidence strength: {evidence_strength}",
               "Analysis completed with limited expert input"
           ],
           "evidence_summary": {
               "total_evidence_items": len(credibility_assessment.get('key_strengths', [])) + len(credibility_assessment.get('key_weaknesses', [])),
               "supporting_evidence_count": len(credibility_assessment.get('key_strengths', [])),
               "contradicting_evidence_count": len(credibility_assessment.get('key_weaknesses', [])),
               "neutral_evidence_count": 0,
               "evidence_quality_score": overall_credibility
           },
           "limitations": [
               "Limited expert analysis",
               "Automated processing only",
               "May require additional verification"
           ],
           "recommendations": [
               "Seek additional expert validation",
               "Review primary sources",
               "Consider alternative perspectives"
           ],
           "confidence_factors": {
               "source_quality": overall_credibility,
               "evidence_consistency": 0.6,
               "expert_consensus": 0.4,
               "methodological_rigor": 0.5
           },
           "verdict_timestamp": datetime.now().isoformat(),
           "analysis_completeness": 0.7
       }

    def _calculate_analysis_completeness(self, analysis_result: Dict[str, Any]) -> float:
       """Calculate overall analysis completeness score"""
       required_stages = ["content_preprocessing", "search_pipeline", "evidence_analysis", 
                         "credibility_assessment", "domain_analysis", "verdict_generation"]
       
       completed_stages = sum(1 for stage in required_stages 
                            if stage in analysis_result.get("pipeline_stages", {}))
       
       base_completeness = completed_stages / len(required_stages)
       
       # Bonus for additional components
       bonus = 0
       if analysis_result.get("image_analysis"):
           bonus += 0.1
       if analysis_result.get("search_results", {}).get("confidence_metrics", {}).get("overall_confidence", 0) > 0.7:
           bonus += 0.1
       
       return min(1.0, base_completeness + bonus)

@app.get("/", response_class=HTMLResponse)
async def read_root():
    return """
    i have the html but its unrelated
    """

